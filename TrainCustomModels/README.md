# Entrenar detectores personalizados (YOLO)

En esta carpeta encontrarÃ¡s un **pipeline completo** para preparar datasets en formato COCO/Roboflow, convertirlos a tablas MATLAB, **entrenar YOLOv4** (incluida la variante *tiny*), evaluar, y luego probar el detector entrenado en imÃ¡genes o **en vivo con webcam**.

---

## ğŸ“‚ Estructura

```text
TrainCustomModels/
â”œâ”€â”€ coco2table.m                 # COCO/Roboflow JSON â†’ tabla MATLAB (bbox + clases)
â”œâ”€â”€ convertDataset.m             # Orquesta la conversiÃ³n por splits (train/valid/test)
â”œâ”€â”€ formatForYOLO.m              # Formateo de muestras para datastores combinados
â”œâ”€â”€ preprocessDataYOLOv4.m       # (Opcional) Resize/augment para flujo alternativo
â”œâ”€â”€ runYOLOv4.m                  # Script de experiencia: entrena y prueba
â”œâ”€â”€ trainBoxesYOLOv4.m           # FunciÃ³n principal de entrenamiento/validaciÃ³n
â”œâ”€â”€ webcamPredict.m              # Inferencia en vivo con webcam sobre detector entrenado
â””â”€â”€ (salidas) checkpoints/, trainedDetectorYOLOv4.mat, curvas P-R, etc.
````

---

## ğŸ§ª Flujo recomendado

1. **Convierte** tu dataset COCO (JSON) a tablas MATLAB: `convertDataset.m`
2. **Entrena** con `runYOLOv4.m` (internamente llama a `trainBoxesYOLOv4`).
3. **Valida/visualiza** resultados y guarda `trainedDetectorYOLOv4.mat`.
4. **Prueba** detecciÃ³n en imÃ¡genes o **en vivo** con `webcamPredict.m`.

---

## ğŸ“Œ Scripts y funciones (entradas/salidas)

### 1) `coco2table.m` â€” COCO JSON â†’ tabla MATLAB

**QuÃ© hace:** Parsea el JSON COCO de Roboflow y construye una **tabla** con:

* `imageFilename` (`string`): ruta absoluta de cada imagen,
* `objectBoundingBoxes` (`cell{MÃ—4 double}`): \[x y w h],
* `objectClass` (`cell{MÃ—1 categorical}`).
  Omite imÃ¡genes sin cajas y normaliza cada bbox a fila 1Ã—4. **Salidas seguras y consistentes** para entrenamiento.&#x20;

**Entradas**

* `jsonFile` (char/string): ruta al `_annotations.coco.json`.
* `imgFolder` (char/string): carpeta con las imÃ¡genes.

**Salida**

* `tbl` (`table`): tabla con rutas, bboxes y etiquetas.

---

### 2) `convertDataset.m` â€” Lanza la conversiÃ³n por *splits*

**QuÃ© hace:** Recorre `train/valid/test`, valida la existencia de `_annotations.coco.json` e imÃ¡genes, llama a `coco2table` y **guarda** en cada carpeta un `.mat` con `tbl` (por ejemplo, `train_cocoTable.mat`).&#x20;

**Entradas**

* Usa `pwd` como raÃ­z; espera `datasets/<proyecto>/{train,valid,test}`.
* Asume nombres estÃ¡ndar: `_annotations.coco.json` + imÃ¡genes.

**Salidas**

* En cada split: `<split>_cocoTable.mat` con variable `tbl`.

---

### 3) `formatForYOLO.m` â€” *Adapter* para datastores

**QuÃ© hace:** Adapta muestras para flujos con `combine(imageDatastore, boxLabelDatastore)`:

* Si la entrada ya viene como `{I, boxes, labels}`, la deja igual.
* Si viene como `{I, tbl}` (o `{I, {tbl}}`), **extrae** `objectBoundingBoxes` y `objectClass`.
  Valida tipos y estandariza el formato final `{I, boxes, labels}`.&#x20;

**Entradas**

* `dataIn`: `{I, boxes, labels}` **o** `{I, tbl}` / `{I, {tbl}}`.

**Salida**

* `dataOut`: `{I, boxes, labels}` listos para el *mini-batch*.

---

### 4) `preprocessDataYOLOv4.m` â€” (Opcional) Resize/augment

**QuÃ© hace:** Ejemplo de *preprocess* sencillo (no usado en el flujo por defecto). Hace `imresize` a un `target` (p. ej., 416Ã—416), ajusta cajas con el *scale* correspondiente, y devuelve `{I_resized, bboxes_rescaled}`. Ãštil si quieres experimentar con un pipeline manual.&#x20;

**Entradas**

* `in`: `{I, b}` donde `b` puede venir anidado en celda.
* `target`: `[H W C]` deseado (p. ej. `[416 416 3]`).

**Salida**

* `out`: `{I_resized, b_resized}`.

---

### 5) `trainBoxesYOLOv4.m` â€” Entrenar (Â¡el nÃºcleo!)

**QuÃ© hace (resumen de pasos):**

1. Carga las tablas `train/valid/test` y **limpia** categorÃ­as vacÃ­as.
2. **Clip/Drop** de cajas fuera del encuadre real de cada imagen.
3. Crea datastores combinados y les aplica `formatForYOLO`.
4. **Estima anchors** (9 por defecto; 6 y *split* 3+3 para *tiny-yolov4*).
5. Define el detector base (**input 416Ã—416Ã—3** por defecto), entrena, evalÃºa y genera curvas P-R.
   AdemÃ¡s, crea/usa una carpeta `checkpoints`.&#x20;

**Entradas**

* `dataDir` (char/string): raÃ­z con `train/valid/test` (cada split con su `*_cocoTable.mat` + imÃ¡genes).
* `checkpointsFolder` (char/string): ruta donde guardar *snapshots*.
* `modelName` (string): `"csp-darknet53-coco"` o `"tiny-yolov4-coco"`.

**Salidas**

* `detector`: objeto `yolov4ObjectDetector` listo para `detect(...)`.
* Archivos auxiliares en `checkpoints/` y mÃ©tricas/figuras (Pâ€“R), segÃºn config.

**Notas clave**

* *Anchors*: ordena por Ã¡rea y divide en grupos (2 grupos si *tiny*, 3 si estÃ¡ndar).&#x20;
* *InputSize*: por defecto `[416 416 3]`; el *resize* se maneja en el *reader*.&#x20;

---

### 6) `runYOLOv4.m` â€” Experimento de punta a punta

**QuÃ© hace:** Define carpetas y el `modelName`, **llama a** `trainBoxesYOLOv4` y luego **hace predicciÃ³n** con el detector entrenado sobre una imagen de *test*, dibujando cajas + scores (ejemplo mÃ­nimo de *inference-after-train*).&#x20;

**Entradas**

* Variables internas: `datasetFolder`, `projectName`, `modelName`.

**Salidas**

* Variable `detector` en el *workspace*, figura con detecciones dibujadas.

---

### 7) `webcamPredict.m` â€” *Live demo* con webcam

**QuÃ© hace:** Carga `trainedDetectorYOLOv4.mat` (variable `detector`), abre la webcam (Support Package), **detecta en vivo** y dibuja `label: score` en cada cuadro. Muestra FPS y permite bajar resoluciÃ³n para ganar rendimiento.&#x20;

**Entradas**

* `trainedDetectorYOLOv4.mat` en la misma carpeta (variable `detector`).

**Salidas**

* Ventana con *preview* y detecciones en tiempo real (FPS en tÃ­tulo).

---

## ğŸ§© Piezas relacionadas (detecciÃ³n e inferencia)

Aunque el foco aquÃ­ es **entrenar**, tambiÃ©n se incluyen utilidades de inferencia genÃ©ricas (Ãºtiles para validar tus modelos):

* `myDetector.m`: detector **CPU**; garantiza RGB, llama a `detect(det, I, Threshold=...)`. **Salidas**: `[bboxes, scores, labels]` (labels como *cellstr*).&#x20;
* `myDetectorGPU.m`: detector **GPU/Codegen** con firma `[b,s,lIdx] = myDetectorGPU(im, detectorFile, threshold)`. Carga el detector una vez, asegura 3 canales y devuelve **Ã­ndices** de clase (1-based).&#x20;
* `myDetectorGPU_generation_script.m`: prepara **codegen** para Jetson (dll + *Embedded Coder*), fija `inputSize` desde el detector, define tipos y genera `myDetectorGPU.so`.&#x20;
* `testDetector.m`: *harness* para comparar CPU/GPU y **normalizar etiquetas** a texto antes de dibujar anotaciones.&#x20;

> Si vas a desplegar en ROS (fuera de esta carpeta), tus nodos C++ publicarÃ¡n:
>
> * `/network_view` (imagen RGB recortada) y
> * `/network_detections` (Float32MultiArray con filas `[x y w h score labelIdx]`), que puedes **visualizar desde MATLAB** con `visualizeROSDetector.m`.&#x20;

---

## ğŸ› ï¸ Consejos prÃ¡cticos

* **Calidad de anotaciones**: ejecuta *clip/drop* (ya incluido) para evitar cajas fuera de imagen y *anchors* mÃ¡s estables.&#x20;
* **Anchors y tamaÃ±os**: usa 416Ã—416Ã—3 como base (o ajusta a tu *backbone*). Estima anchors tras el *resize* esperado.&#x20;
* **`formatForYOLO`** te quita dolores de cabeza con las combinaciones `{I, tbl}` â†’ `{I, boxes, labels}` en *mini-batches*.&#x20;
* **Webcam**: baja resoluciÃ³n si tu cÃ¡mara es 1080p/4K; ganarÃ¡s FPS.&#x20;

---

## â–¶ï¸ Ejemplos rÃ¡pidos

### Entrenar (fin a fin) y probar una imagen

```matlab
% 1) AsegÃºrate de tener datasets/<proyecto>/{train,valid,test}
convertDataset        % crea train_cocoTable.mat, etc.

% 2) Lanza experimento
runYOLOv4             % entrena y prueba sobre una imagen de test
```

&#x20;

### Cargar detector y detectar en vivo (webcam)

```matlab
% tras el entrenamiento, asegÃºrate de tener trainedDetectorYOLOv4.mat
webcamPredict
```



---

## ğŸ“¦ Salidas tÃ­picas

* `trainedDetectorYOLOv4.mat` (variable `detector` lista para `detect(...)`).&#x20;
* Carpeta `checkpoints/<proyecto>/` con snapshots y logs de entrenamiento.&#x20;
* Figuras/curvas de evaluaciÃ³n (Pâ€“R, *mean IoU* de anchors, etc.).&#x20;

```

Â¿Quieres que deje al final una **secciÃ³n breve de â€œProblemas comunes y solucionesâ€** (por ejemplo: â€œno encuentra el JSONâ€, â€œanchors rarosâ€, â€œout of memoryâ€)? La agrego en un periquete.
```


AquÃ­ tienes el **README para la carpeta TrainCustomModels**, listo para copiar y pegar. Explica de forma clara quÃ© hace cada archivo, sus entradas y salidas, y cÃ³mo usarlos juntos para entrenar y probar un detector YOLOv4 personalizado en MATLAB.

````markdown
# Entrenar detectores personalizados con YOLOv4

Esta carpeta proporciona un **pipeline completo** para crear tu propio detector de objetos con YOLOv4 (incluyendo la variante *tiny*) a partir de datos anotados en formato COCO/Roboflow. Los scripts convierten JSON a tablas MATLAB, preparan los datos, entrenan el modelo, lo evalÃºan y permiten probarlo en imÃ¡genes individuales o en vivo mediante webcam.

---

## ğŸ“‚ Estructura de la carpeta

```text
TrainCustomModels/
â”œâ”€â”€ coco2table.m                # COCO/Roboflow JSON â†’ tabla MATLAB (bbox + clases)
â”œâ”€â”€ convertDataset.m            # Orquesta la conversiÃ³n por splits (train/valid/test)
â”œâ”€â”€ formatForYOLO.m             # Formateo de muestras para datastores combinados
â”œâ”€â”€ preprocessDataYOLOv4.m      # (Opcional) Resize/augment para flujo alternativo
â”œâ”€â”€ runYOLOv4.m                 # Script de experiencia: entrena y prueba
â”œâ”€â”€ trainBoxesYOLOv4.m          # FunciÃ³n principal de entrenamiento y evaluaciÃ³n
â”œâ”€â”€ webcamPredict.m             # Inferencia en vivo con webcam sobre detector entrenado
â””â”€â”€ (salidas)                   # Checkpoints, trainedDetectorYOLOv4.mat, curvas P-R, etc.
````

---

## ğŸ› ï¸ PreparaciÃ³n del dataset

### `coco2table.m` â€” COCO JSON â†’ tabla MATLAB

Convierte un archivo JSON de anotaciones COCO/Roboflow en una **tabla MATLAB** con tres columnas: `imageFilename` (ruta absoluta de la imagen), `objectBoundingBoxes` (celda de MÃ—4 `[x y w h]` para cada imagen) y `objectClass` (celda de M etiquetas como `categorical`). Omite imÃ¡genes sin cajas y garantiza que cada bbox sea una fila 1Ã—4.
**Entradas:** `jsonFile` (ruta al `_annotations.coco.json`), `imgFolder` (carpeta de imÃ¡genes).
**Salida:** `tbl` (tabla con rutas, bboxes y etiquetas).

### `convertDataset.m` â€” ConversiÃ³n por splits

Automatiza la conversiÃ³n de todo tu dataset. Recorre las carpetas `train`, `valid` y `test`, verifica la existencia del JSON y las imÃ¡genes, llama a `coco2table` y **guarda** en cada carpeta un archivo `<split>_cocoTable.mat` con la tabla correspondiente.
**Entradas:** se define `dataDir` con estructura `datasets/<proyecto>/{train,valid,test}`.
**Salidas:** `train_cocoTable.mat`, `valid_cocoTable.mat`, `test_cocoTable.mat`.

---

## ğŸ§° Formateo para YOLO

### `formatForYOLO.m` â€” Adaptador para datastores

Convierte lotes `{I, boxes, labels}` o `{I, tbl}` a `{I, boxes, labels}`. Si la entrada es `{I, tbl}` (o `{I, {tbl}}`), extrae `objectBoundingBoxes` y `objectClass` de la tabla; si ya viene en forma `{I, boxes, labels}`, no hace cambios.
**Entrada:** celda `dataIn` con 2 o 3 elementos.
**Salida:** celda `{I, boxes, labels}` lista para `combine`/`transform`.

### `preprocessDataYOLOv4.m` â€” Resize/augment (opcional)

Ejemplo de preprocesado: hace `imresize` a un tamaÃ±o objetivo (`target`), convierte la imagen a `single`, reescala las cajas de acuerdo con el factor de resize y devuelve `{I_resized, bboxes_rescaled}`.
**Entradas:** `in = {I, b}` (imagen y cajas), `target = [H W C]`.
**Salida:** `out = {I_resized, b_resized}`.

---

## ğŸ“ Entrenamiento y evaluaciÃ³n

### `trainBoxesYOLOv4.m` â€” NÃºcleo del entrenamiento

FunciÃ³n que entrena y evalÃºa un detector YOLOv4 (MATLAB R2023a o posterior). Pasos principales:

1. **Carga y limpieza de tablas**: lee `train/valid/test_cocoTable.mat` de `dataDir` y elimina categorÃ­as vacÃ­as.
2. **Recorte/Descarte de cajas**: recorta las cajas al tamaÃ±o de la imagen real o descarta cajas fuera del encuadre; muestra cuÃ¡ntas cajas se eliminaron por split.
3. **CreaciÃ³n de datastores**: combina `imageDatastore` y `boxLabelDatastore`, aplicando `formatForYOLO` para obtener `{I, boxes, labels}`.
4. **EstimaciÃ³n de anchors**: redimensiona las cajas a 416Ã—416 y estima anchors (9 por defecto). Para el modelo *tiny*, calcula 6 anchors y los divide en dos grupos de 3.
5. **DefiniciÃ³n del detector base**: usa `yolov4ObjectDetector(modelName, classNames, anchorBoxes, InputSize=[416 416 3])` para crear el modelo.
6. **Opciones de entrenamiento**: configura `trainingOptions("adam", ...)` con parÃ¡metros de mini-batch, tasa de aprendizaje inicial, nÃºmero de Ã©pocas (120 por defecto), regularizaciÃ³n L2, uso de GPU, validaciÃ³n, checkpoints y grÃ¡fica de progreso.
7. **Entrenamiento**: llama a `trainYOLOv4ObjectDetector` para entrenar y guarda `trainedDetectorYOLOv4.mat`.
8. **EvaluaciÃ³n**: detecta objetos en `dsTest`, calcula *mAP\@0.5*, `recall` y `precision` y lo muestra en pantalla. TambiÃ©n genera curvas PrecisiÃ³nâ€“Recall para cada clase.

**Entradas:**

* `dataDir`: carpeta base con `train`, `valid`, `test` (cada split con su .mat y sus imÃ¡genes).
* `checkpointsFolder`: carpeta donde se guardan los modelos intermedios.
* `modelName`: cadena (`"csp-darknet53-coco"` para el modelo completo, `"tiny-yolov4-coco"` para la variante ligera).

**Salidas:**

* `detector`: objeto `yolov4ObjectDetector` listo para inferencia.
* Archivos en `checkpointsFolder`, mÃ©tricas e imÃ¡genes generadas.

---

### `runYOLOv4.m` â€” Experimento de punta a punta

Script que sirve de plantilla rÃ¡pida para entrenar y probar el detector:

1. **Define** rutas (`datasetFolder`, `projectName`), elige el modelo (`modelName` = `"tiny-yolov4-coco"` por defecto) y crea la carpeta de checkpoints.
2. **Entrena**: llama a `trainBoxesYOLOv4(dataDir, checkpointsFolder, modelName)`.
3. **Prueba**: lee una imagen de la carpeta `test`, ejecuta `detect(detector,I,Threshold=0.20)` y dibuja las cajas con sus puntuaciones.

Es ideal para replicar rÃ¡pidamente el proceso con tus propios datos; basta cambiar `projectName` y `modelName`.

---

### `webcamPredict.m` â€” Demo en vivo

Permite probar el detector entrenado en tiempo real usando una webcam. Pasos principales:

1. **Carga** el archivo `trainedDetectorYOLOv4.mat` (variable `detector`).
2. **Conecta** a la webcam y ajusta su resoluciÃ³n a la mÃ¡s baja disponible para aumentar FPS.
3. **Bucle principal**: captura una imagen (`snapshot(cam)`), ejecuta `detect(detector,I,Threshold=0.70)`, dibuja las cajas con `insertObjectAnnotation` y muestra FPS actualizado.
4. **Salida**: cierra al pulsar Q y libera la webcam.

Perfecto para demostrar rÃ¡pidamente el rendimiento del detector y ajustar el umbral de confianza.

---

## ğŸ“¦ Salidas tÃ­picas

Al ejecutar el pipeline se generan:

* `trainedDetectorYOLOv4.mat`: archivo MATLAB con la variable `detector` lista para `detect(...)`.
* Carpeta `checkpoints/<proyecto>/`: snapshots del entrenamiento y logs, Ãºtiles para reanudar o analizar.
* Figuras de evaluaciÃ³n (curvas PrecisiÃ³nâ€“Recall, IoU de anchors, etc.).
* `*_cocoTable.mat` en cada split de tu dataset (train/valid/test) con las tablas generadas.

---

## ğŸ’¡ Consejos prÃ¡cticos

* **Anota bien**: revisa que tus archivos JSON COCO y las imÃ¡genes estÃ©n correctamente alineados antes de convertirlos.
* **Anchors y resoluciÃ³n**: usa 416Ã—416Ã—3 como tamaÃ±o de entrada por defecto; ajusta anchors si cambias la resoluciÃ³n base.
* **Calidad de las cajas**: el script recorta o descarta cajas fuera del encuadre y evita cajas de 1 px para robustez.
* **Webcam**: reduce la resoluciÃ³n de la webcam para mejorar la velocidad de inferencia.
* **Tiny vs. full**: la variante *tiny-yolov4-coco* usa 6 anchors (dos grupos de 3) y es mÃ¡s ligera; el modelo completo usa 9 anchors (tres grupos).